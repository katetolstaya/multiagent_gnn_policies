[DEFAULT]
alg = dagger

# learning parameters
batch_size = 30
buffer_size = 10000
updates_per_step = 200
seed = 11
actor_lr = 5e-5

n_train_episodes = 400
beta_coeff = 0.993
test_interval = 40
n_test_episodes = 20

# architecture parameters
k = 3
hidden_size = 32
gamma = 0.99
tau = 0.1

# env parameters
env = FlockingStochastic-v0
v_max = 0.5
comm_radius = 1.15
n_agents = 50
n_actions = 2
n_states = 6
debug = True


header = k, comm_radius, reward



[2, 1.25]
k = 2
comm_radius = 1.25

[2, 1.15]
k = 2
comm_radius = 1.15

[2, 1.0]
k = 2
comm_radius = 1.0

[3, 1.25]
k = 3
comm_radius = 1.25

[3, 1.15]
k = 3
comm_radius = 1.15

[3, 1.0]
k = 3
comm_radius = 1.0

[1, 1.25]
k = 1
comm_radius = 1.25

[1, 1.15]
k = 1
comm_radius = 1.15

[1, 1.0]
k = 1
comm_radius = 1.0


[4, 1.25]
k = 4
comm_radius = 1.25

[4, 1.15]
k = 4
comm_radius = 1.15

[4, 1.0]
k = 4
comm_radius = 1.0

