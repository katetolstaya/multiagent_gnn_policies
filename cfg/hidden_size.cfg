[DEFAULT]

alg = dagger

# learning parameters
batch_size = 20
buffer_size = 10000
updates_per_step = 200
seed = 11
actor_lr = 5e-5

n_train_episodes = 3200
beta_coeff = 0.993
test_interval = 40
n_test_episodes = 20

# architecture parameters
k = 3
hidden_size = 32
gamma = 0.99
tau = 0.5

# env parameters
env = FlockingRelative-v0
v_max = 3.0
comm_radius = 1.0
n_agents = 100
n_actions = 2
n_states = 6
debug = False
dt = 0.01


header = n_layers, hidden_size, reward

[1, 4]
n_layers = 1
hidden_size = 4

[1, 8]
n_layers = 1
hidden_size = 8

[1, 16]
n_layers = 1
hidden_size = 16

[1, 32]
n_layers = 1
hidden_size = 32

[1, 64]
n_layers = 1
hidden_size = 64

[1, 128]
n_layers = 1
hidden_size = 128

[2, 4]
n_layers = 2
hidden_size = 4

[2, 8]
n_layers = 2
hidden_size = 8

[2, 16]
n_layers = 2
hidden_size = 16

[2, 32]
n_layers = 2
hidden_size = 32

[2, 64]
n_layers = 2
hidden_size = 64

[2, 128]
n_layers = 2
hidden_size = 128

[3, 4]
n_layers = 3
hidden_size = 4

[3, 8]
n_layers = 3
hidden_size = 8

[3, 16]
n_layers = 3
hidden_size = 16

[3, 32]
n_layers = 3
hidden_size = 32

[3, 64]
n_layers = 3
hidden_size = 64

[3, 128]
n_layers = 3
hidden_size = 128

[4, 4]
n_layers = 4
hidden_size = 4

[4, 8]
n_layers = 4
hidden_size = 8

[4, 16]
n_layers = 4
hidden_size = 16

[4, 32]
n_layers = 4
hidden_size = 32

[4, 64]
n_layers = 4
hidden_size = 64

[4, 128]
n_layers = 4
hidden_size = 128

