[DEFAULT]

alg = dagger

# learning parameters
batch_size = 20
buffer_size = 10000
updates_per_step = 200
seed = 11
actor_lr = 5e-5

n_train_episodes = 400
beta_coeff = 0.993
test_interval = 40
n_test_episodes = 20

# architecture parameters
k = 2
hidden_size = 32
gamma = 0.99
tau = 0.5

# env parameters
env = FlockingRelative-v0
v_max = 3.0
comm_radius = 1.0
n_agents = 100
n_actions = 2
n_states = 6
debug = False
dt = 0.01


header = v_max, k, reward

[5.5, 1]
v_max = 5.5
k = 1

[5.5, 2]
v_max = 5.5
k = 2

[5.5, 3]
v_max = 5.5
k = 3

[5.5, 4]
v_max = 5.5
k = 4

[0.5, 1]
v_max = 0.5
k = 1

[0.5, 2]
v_max = 0.5
k = 2

[0.5, 3]
v_max = 0.5
k = 3

[0.5, 4]
v_max = 0.5
k = 4

[1.5, 1]
v_max = 1.5
k = 1

[1.5, 2]
v_max = 1.5
k = 2

[1.5, 3]
v_max = 1.5
k = 3

[1.5, 4]
v_max = 1.5
k = 4

[2.5, 1]
v_max = 2.5
k = 1

[2.5, 2]
v_max = 2.5
k = 2

[2.5, 3]
v_max = 2.5
k = 3

[2.5, 4]
v_max = 2.5
k = 4

[3.5, 1]
v_max = 3.5
k = 1

[3.5, 2]
v_max = 3.5
k = 2

[3.5, 3]
v_max = 3.5
k = 3

[3.5, 4]
v_max = 3.5
k = 4

[4.5, 1]
v_max = 4.5
k = 1

[4.5, 2]
v_max = 4.5
k = 2

[4.5, 3]
v_max = 4.5
k = 3

[4.5, 4]
v_max = 4.5
k = 4

