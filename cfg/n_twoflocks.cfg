[DEFAULT]

alg = dagger

# learning parameters
batch_size = 20
buffer_size = 5000
updates_per_step = 200
seed = 12
actor_lr = 5e-5

n_train_episodes = 400
beta_coeff = 0.993
test_interval = 40
n_test_episodes = 20

# architecture parameters
k = 2
hidden_size = 32
gamma = 0.99
tau = 0.5

# env parameters
env = FlockingTwoFlocks-v0
v_max = 3.0
comm_radius = 1.0
n_agents = 100
n_actions = 2
n_states = 6
debug = False
dt = 0.01


header = k, n_agents, reward



[1, 50]
k = 1
n_agents = 50

[1, 100]
k = 1
n_agents = 100

[1, 150]
k = 1
n_agents = 150

[1, 200]
k = 1
n_agents = 200

[1, 250]
k = 1
n_agents = 250


[2, 50]
k = 2
n_agents = 50

[2, 100]
k = 2
n_agents = 100

[2, 150]
k = 2
n_agents = 150

[2, 200]
k = 2
n_agents = 200

[2, 250]
k = 2
n_agents = 250



[3, 50]
k = 3
n_agents = 50

[3, 100]
k = 3
n_agents = 100

[3, 150]
k = 3
n_agents = 150

[3, 200]
k = 3
n_agents = 200

[3, 250]
k = 3
n_agents = 250


[4, 50]
k = 4
n_agents = 50

[4, 100]
k = 4
n_agents = 100

[4, 150]
k = 4
n_agents = 150

[4, 200]
k = 4
n_agents = 200

[4, 250]
k = 4
n_agents = 250





